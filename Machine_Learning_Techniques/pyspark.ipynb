{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you connect to a Spark cluster from PySpark?\n",
    "SparkContext as your connection to the cluster \n",
    "SparkSession as your interface with that connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession.builder.getOrCreate() method returns an existing SparkSession \n",
    "1. if there's already one in the environment, \n",
    "2. or creates a new one if necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "# Print my_spark\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this query\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandafy a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the airports data\n",
    "airports = spark.read.csv(file_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DataFrame flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame flights\n",
    "flights = spark.table('flights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"newCol\", df.oldCol + 1)\n",
    "# Rename the faa column\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".filter() method is the Spark counterpart of SQL's WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter flights by passing a string\n",
    "long_flights1 = flights.filter(\"distance > 1000\")\n",
    "# Filter flights by passing a column of boolean values\n",
    "long_flights2 = flights.filter(flights.distance > 1000)\n",
    "# Filter the data, first by filterA then by filterB\n",
    "selected2 = temp.filter(flights.origin == \"SEA\").filter(flights.dest == \"PDX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first set of columns\n",
    "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
    "# Select the second set of columns\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the shortest flight from PDX in terms of distance\n",
    "flights.filter(flights.origin == \"PDX\").groupBy().min('distance').show()\n",
    "\n",
    "# Find the longest flight from SEA in terms of air time\n",
    "flights.filter(flights.origin == \"SEA\").groupBy().max('air_time').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines //pyspark.ml\n",
    "1. Transformer classes have a .transform() method \n",
    "    takes a DataFrame and returns a new DataFrame\n",
    "2. Estimator classes all implement a .fit() method\n",
    "    These methods also take a DataFrame, return a model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data type\n",
    "Spark only handles numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the type of a column using the .cast() method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the columns to integers\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings and factors //one-hot vector\n",
    "pyspark.ml.features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create a StringIndexer <br />\n",
    "Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. <br />\n",
    "the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.\n",
    "2. encode this numeric column as a one-hot vector using a OneHotEncoder\n",
    "3. The end result is a column that encodes your categorical feature as a vector that's suitable for machine learning routines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol=\"carrier\",outputCol=\"carrier_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\" , outputCol=\"carrier_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "330204c9d8b03d73c01b77e5fa072b00779363ad44c9b37d26bce407274881dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
